{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1249c874",
   "metadata": {},
   "source": [
    "# Job Posting Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffbabd9a",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e02bb3bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup \n",
    "from os import walk\n",
    "import pymongo\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69b98eb",
   "metadata": {},
   "source": [
    "### General Fucntions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "25aa1e90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdriver ():\n",
    "    path=r'chromedriver.exe'\n",
    "    #driver = webdriver.Chrome(executable_path='/Users/Sripriya Srinivasan/Downloads/chromedriver_win32/chromedriver')\n",
    "    driver = webdriver.Chrome(executable_path=path)\n",
    "    driver.implicitly_wait(10)\n",
    "    driver.set_script_timeout(120)\n",
    "    driver.set_page_load_timeout(30)\n",
    "    return driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5cb5794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writePage(fname,content):\n",
    "    with open(fname, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(str(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c0b9df59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file (name):\n",
    "    HTMLFile = open(name, \"rb\")\n",
    "    htmlfiledata = HTMLFile.read()\n",
    "    return BeautifulSoup(htmlfiledata, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "503a1d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_mongodb (db_name, collection_name):\n",
    "    client = pymongo.MongoClient(\"mongodb://localhost:27017\")\n",
    "    db = client[db_name]\n",
    "    collection = db[collection_name]\n",
    "    return collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49ead3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWebsiteData (url):\n",
    "    try:\n",
    "        headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "        page = requests.get(url,headers=headers)\n",
    "        # Create a beautifulsoup object \n",
    "        return BeautifulSoup(page.text, 'lxml')\n",
    "    except :\n",
    "        print(\"Error connecting to website\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a627af06",
   "metadata": {},
   "source": [
    "## LinkedIn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387384f2",
   "metadata": {},
   "source": [
    "### Infinite Scroll and Save all results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6691fbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scroll_and_save(url,role):\n",
    "    files_list = []\n",
    "    driver = getdriver()\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  \n",
    "    scroll_pause_time = 1 \n",
    "    screen_height = driver.execute_script(\"return window.screen.height;\")\n",
    "    i = 1\n",
    "    link_num = 1\n",
    "    while True:\n",
    "        driver.execute_script(\"window.scrollTo(0, {screen_height}*{i});\".format(screen_height=screen_height, i=i))  \n",
    "        i += 1\n",
    "        time.sleep(scroll_pause_time)\n",
    "        scroll_height = driver.execute_script(\"return document.body.scrollHeight;\") \n",
    "        try:\n",
    "            button = driver.find_elements(\"xpath\",\"//*[contains(text(), 'See more jobs')]\")\n",
    "            button[0].click()\n",
    "        except:\n",
    "            print(\"Button not found\")\n",
    "        if (screen_height) * i > scroll_height:\n",
    "            break\n",
    "    writePage(role+'_linkedin_search_results',driver.page_source)\n",
    "    links = driver.find_elements(\"css selector\",\"a.base-card__full-link.absolute\")\n",
    "    \n",
    "    for link in links:\n",
    "        fname = role+'_'+str(link_num)\n",
    "        files_list.append(fname)\n",
    "        time.sleep(2)\n",
    "        writePage(fname,loadWebsiteData(link.get_attribute('href')))\n",
    "        link_num = link_num + 1\n",
    "\n",
    "    print(len(links))\n",
    "    driver.close()\n",
    "    return files_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873fe3cc",
   "metadata": {},
   "source": [
    "### Iterate through saved files and parse information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cdd91c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_and_save(files_list):\n",
    "    job_posting_list = []\n",
    "    collection = connect_mongodb ('job_postings', 'job_postings_data')\n",
    "    for file in files_list :\n",
    "        job_soup = read_file(\"ba/\"+file)\n",
    "        role = job_soup.select(\"h1\")[0].text if len(job_soup.select(\"h1\"))>0 else 'NA'\n",
    "        company = job_soup.select(\"a.topcard__org-name-link\")[0].text.strip() if len(job_soup.select(\"a.topcard__org-name-link\"))>0 else 'NA'\n",
    "        location =  job_soup.select(\"span.topcard__flavor.topcard__flavor--bullet\")[0].text.strip() if len(job_soup.select(\"span.topcard__flavor.topcard__flavor--bullet\"))>0 else 'NA'\n",
    "        info = job_soup.select(\"div.show-more-less-html__markup\")[0].text.strip() if len(job_soup.select(\"div.show-more-less-html__markup\"))>0 else 'NA'\n",
    "\n",
    "        role_code = '3' #'DS'=1,'DA'=2,'BA'=3\n",
    "        \n",
    "        fields = job_soup.select(\"h3.description__job-criteria-subheader\")\n",
    "        values = job_soup.select(\"span.description__job-criteria-text.description__job-criteria-text--criteria\")\n",
    "\n",
    "        extra_info = {}\n",
    "\n",
    "        js = job_soup.findAll('script',type='application/ld+json')\n",
    "        if(len(js)>0):\n",
    "            skills = []\n",
    "            education = []\n",
    "            salary_est = 'NA'\n",
    "            years_exp = 'NA'\n",
    "\n",
    "            json_script = json.loads(js[0].string)\n",
    "            if 'skills' in json_script.keys():\n",
    "                skills = json_script['skills']\n",
    "                extra_info['Skills']=skills\n",
    "\n",
    "            if 'educationRequirements' in json_script.keys():\n",
    "                education = json_script['educationRequirements']['credentialCategory'] if 'credentialCategory' in json_script['educationRequirements'].keys() else 'NA'\n",
    "                extra_info['Education']=education\n",
    "\n",
    "            if 'experienceRequirements' in json_script.keys():\n",
    "                if(isinstance(json_script['experienceRequirements'], str)):\n",
    "                    if(json_script['experienceRequirements']=='no requirements'):\n",
    "                        years_exp = 0\n",
    "                    else:\n",
    "                        years_exp='NA'\n",
    "                else:\n",
    "                    years_exp = json_script['experienceRequirements']['monthsOfExperience']/12 if 'monthsOfExperience' in json_script['experienceRequirements'].keys() else 'NA'\n",
    "                extra_info['Years of Experience']= years_exp\n",
    "\n",
    "            if 'baseSalary' in json_script.keys():\n",
    "                salaryinfo = json_script['baseSalary']['value']\n",
    "                salary_est = '$'+str(salaryinfo['minValue']) +' - '+ '$'+str(salaryinfo['maxValue'])+'/'+salaryinfo['unitText']\n",
    "                extra_info['Salary Estimate']=salary_est\n",
    "\n",
    "        for i in range(0,len(fields)):\n",
    "            extra_info[fields[i].text.strip()] = values[i].text.strip()\n",
    "\n",
    "        posting = {'Job Role':role,'Role Code':role_code,'Company':company,'Location':location,'Job Description':info,'Additional Details':extra_info}\n",
    "\n",
    "        print(file)\n",
    "        print(json.dumps(posting,indent = 4))\n",
    "\n",
    "        job_posting_list.append(posting)\n",
    "\n",
    "    #Remaining postings\n",
    "    collection.insert_many(job_posting_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08792084",
   "metadata": {},
   "outputs": [],
   "source": [
    "datascience_url = \"https://www.linkedin.com/jobs/search?keywords=data%20scientist&location=california&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\"\n",
    "dataanalyst_url = \"https://www.linkedin.com/jobs/search/?currentJobId=3489147354&geoId=102095887&keywords=Data%20Analyst&location=California%2C%20United%20States&refresh=true\"\n",
    "businessanalyst_url = \"https://www.linkedin.com/jobs/search?keywords=business%20analyst&location=california&geoId=&trk=public_jobs_jobs-search-bar_search-submit&position=1&pageNum=0\"\n",
    "\n",
    "parse_and_save(scroll_and_save(datascience_url,'ds'))\n",
    "parse_and_save((scroll_and_save(dataanalyst_url,'da'))\n",
    "parse_and_save((scroll_and_save(businessanalyst_url,'ba'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48aa80e",
   "metadata": {},
   "source": [
    "### Glassdoor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc4a973",
   "metadata": {},
   "source": [
    "### Go to all 30 search pages and save job postings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "75a9488a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_jobs_from_glassdoor(url, prefix = 'data', n=20000):    \n",
    "    pause_time = .2 \n",
    "    n_per_page = 30 # default \n",
    "    driver = getdriver()\n",
    "    driver_temp = getdriver()  \n",
    "    driver.get(url)\n",
    "    \n",
    "    time.sleep(pause_time)\n",
    "    for page in range(1,n_per_page+1): \n",
    "        ## Click 'x' on log-in popup        \n",
    "        try:\n",
    "            dummy_job = driver.find_element(\"css selector\", \"a.jobLink.css-1rd3saf.eigr9kq2\").click()  \n",
    "        except ElementClickInterceptedException:\n",
    "            pass    \n",
    "\n",
    "        time.sleep(pause_time)\n",
    "        try:\n",
    "            x_button = driver.find_element(\"xpath\", \"//*[@id='JAModal']/div/div[2]/span\").click() \n",
    "        except NoSuchElementException:\n",
    "            pass\n",
    "        \n",
    "        ## Get job links\n",
    "        time.sleep(pause_time)\n",
    "        job_buttons = driver.find_elements(\"css selector\", \"a.jobLink.css-1rd3saf.eigr9kq2\")\n",
    "        \n",
    "        ## Save pages        \n",
    "        i = 0\n",
    "        for button in job_buttons:          \n",
    "            i += 1\n",
    "            \n",
    "            time.sleep(pause_time)            \n",
    "            try:\n",
    "                button.click()\n",
    "            except:\n",
    "                print(f\"try again(button.click): {prefix}_page_{page}_job_{i}\")\n",
    "                time.sleep(pause_time*5)\n",
    "                button.click()\n",
    "                \n",
    "            detail_page_url = button.get_attribute(\"href\")           \n",
    "\n",
    "            time.sleep(pause_time)            \n",
    "            try:\n",
    "                driver_temp.get(detail_page_url)\n",
    "            except:\n",
    "                print(f\"try again(driver_temp.get): {prefix}_page_{page}_job_{i}\")\n",
    "                time.sleep(pause_time*5)\n",
    "                driver_temp.get(detail_page_url)\n",
    "            \n",
    "            time.sleep(pause_time)            \n",
    "            f_nm = f\"{prefix}_page_{page}_job_{i}.html\"\n",
    "            print(f_nm)        \n",
    "            writePage(f_nm, driver_temp.page_source)    \n",
    "            \n",
    "            # for small size test\n",
    "            c = (page-1)*n_per_page + i            \n",
    "            if c >= n: #\n",
    "                return\n",
    "        \n",
    "        ## Go to next page after n_per_page jobs\n",
    "        time.sleep(pause_time)\n",
    "        try:\n",
    "            next_button = driver.find_element(\"css selector\", \"button.nextButton.css-1hq9k8.e13qs2071\").click()\n",
    "        except:\n",
    "            print(f\"try again(next_button): {prefix}_page_{page}_job_{i}\")\n",
    "            time.sleep(pause_time*5)\n",
    "            next_button = driver.find_element(\"css selector\", \"button.nextButton.css-1hq9k8.e13qs2071\").click()\n",
    "            \n",
    "\n",
    "    driver.quit()\n",
    "    driver_temp.quit()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b0aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "datascience_url = \"https://www.glassdoor.com/Job/california-data-science-jobs-SRCH_IL.0,10_IS2280_KO11,27.htm\"\n",
    "dataanalyst_url = \"https://www.glassdoor.com/Job/california-data-analyst-jobs-SRCH_IL.0,10_IS2280_KO11,27.htm\"\n",
    "businessanalyst_url = \"https://www.glassdoor.com/Job/california-business-analyst-jobs-SRCH_IL.0,10_IS2280_KO11,27.htm\"\n",
    "\n",
    "get_jobs_from_glassdoor(businessanalyst_url, prefix = 'ba')\n",
    "get_jobs_from_glassdoor(datascience_url, prefix = 'ds')\n",
    "get_jobs_from_glassdoor(dataanalyst_url, prefix = 'da')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48ee63a",
   "metadata": {},
   "source": [
    "### Parse through saved pages and extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0f5c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_job_info(file_list, verbose = True):\n",
    "    job_posting_dict = []\n",
    "    for file in file_list:\n",
    "        print(f\"reading: {file} ...\")\n",
    "        job_soup = read_file(\"ba_pages/\"+file)        \n",
    "           \n",
    "        role_code = '3' #'DS'=1,'DA'=2,'BA'=3\n",
    "        \n",
    "        ## Find <script> tag\n",
    "        def _has_no_attrs(tag):\n",
    "            return tag.name == 'script' and not tag.attrs     \n",
    "        js = job_soup.find(_has_no_attrs).text.replace(\"window.appCache=\", \"\").replace(\";\", \"\")        \n",
    "        json_script = json.loads(js)\n",
    "                \n",
    "        ## Extract\n",
    "        try:\n",
    "            role = job_soup.select_one(\"div[data-test='job-title']\").find(text = True)\n",
    "        except:\n",
    "            role = -1\n",
    "            \n",
    "        try:\n",
    "            company = job_soup.select_one(\"div[data-test='employer-name']\").find(text = True)\n",
    "        except:\n",
    "            company = -1     \n",
    "            \n",
    "        try:\n",
    "            location = job_soup.select_one(\"span[data-test='location']\").find(text = True)\n",
    "        except:\n",
    "            location = -1  \n",
    "            \n",
    "        try:\n",
    "            salary_est = job_soup.select_one(\"span.small.css-10zcshf.e1v3ed7e1\").find(text = True, recursive = False).strip()\n",
    "        except:\n",
    "            salary_est = -1\n",
    "        \n",
    "        try:\n",
    "            education = json_script[\"initialState\"][\"jlData\"][\"header\"][\"indeedJobAttribute\"][\"educationLabel\"]\n",
    "        except:\n",
    "            education = -1\n",
    "            \n",
    "        try:\n",
    "            skill = json_script[\"initialState\"][\"jlData\"][\"header\"][\"indeedJobAttribute\"][\"skillsLabel\"]\n",
    "        except:\n",
    "            skill = -1\n",
    "        \n",
    "        try:\n",
    "            years_exp = json_script[\"initialState\"][\"jlData\"][\"header\"][\"indeedJobAttribute\"][\"yearsOfExperienceLabel\"]\n",
    "        except:\n",
    "            years_exp = -1\n",
    "            \n",
    "        try:\n",
    "            info = re.sub(\"<.*?>\", \" \", json_script[\"initialState\"][\"jlData\"][\"job\"][\"description\"])\n",
    "        except:\n",
    "            info = -1\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"\\n<< File Name: {file} >>\")\n",
    "            print(f\" - Role: {role}\")\n",
    "            print(f\" - Company: {company}\")\n",
    "            print(f\" - Location: {location}\")\n",
    "            print(f\" - Salary Estimate: {salary_est}\")\n",
    "            print(f\" - Education: {education}\")\n",
    "            print(f\" - Skills: {skill}\")\n",
    "            print(f\" - Years of Experience: {years_exp}\")\n",
    "            print(f\" - Job Description: {info}\")\n",
    "        \n",
    "        extra_info = {'Salary Estimate':salary_est,\n",
    "                      'Education':education,\n",
    "                      'Skills':skill,\n",
    "                      'Years of Experience':years_exp}\n",
    "                      \n",
    "        posting = {'Job Role':role,\n",
    "                   'Role Code':role_code,\n",
    "                   'Company':company,\n",
    "                   'Location':location,\n",
    "                   'Job Description':info,\n",
    "                   'Additional Details':extra_info}\n",
    "        \n",
    "        print(json.dumps(posting,indent = 4))   \n",
    "        job_posting_dict.append(posting)\n",
    "        \n",
    "    return job_posting_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd0555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = []\n",
    "## Change path\n",
    "for (dirpath, dirnames, filenames) in walk('C:/Users/Sripriya Srinivasan/Downloads/job-recommender-system/ba'):\n",
    "    file_list.extend(filenames)\n",
    "    break\n",
    "\n",
    "# file_list is a list of saved html files from search results\n",
    "job_posting_dict = extract_job_info(file_list, verbose = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4c8281",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Store in mongoDB\n",
    "collection = connect_mongodb ('job_postings', 'job_postings_data')\n",
    "collection.insert_many(job_posting_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
